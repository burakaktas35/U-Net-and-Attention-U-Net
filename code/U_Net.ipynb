{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i74C56eG9_TS"
      },
      "source": [
        "#**Drive Mount**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NhswwhbA0az"
      },
      "outputs": [],
      "source": [
        "#mount drive\n",
        "%cd ..\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# this creates a symbolic link so that now the path /content/gdrive/My\\ Drive/ is equal to /mydrive\n",
        "!ln -s /content/gdrive/My\\ Drive/ /mydrive\n",
        "\n",
        "# list the contents of /mydrive\n",
        "!ls /mydrive\n",
        "#Navigate to /mydrive/Modern_Computer_Vision/\n",
        "\n",
        "%cd /mydrive/Modern_Computer_Vision/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beZdg6EXAp66"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "# from segmentation import build_unet, vgg16_unet, vgg19_unet, resnet50_unet, inception_resnetv2_unet, densenet121_unet\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9F14tOTRHMfN"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H15n4S1jHNd8"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXVJXSo595Ep"
      },
      "source": [
        "#**GPU Check**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEPT9ThGh99t"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfNIjHPSh-Fb"
      },
      "outputs": [],
      "source": [
        "tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJ5bPFabiDLt"
      },
      "outputs": [],
      "source": [
        "if tf.test.gpu_device_name():\n",
        "  print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
        "else:\n",
        "  print(\"Please install GPU version of TF\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCFrgMgv-3Ra"
      },
      "outputs": [],
      "source": [
        "tf.config.experimental.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fOC-0y2C_wT"
      },
      "source": [
        "#**CPU - GPU Usage Check**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQCrdAeRCwRj"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GForKiv-6Mq"
      },
      "outputs": [],
      "source": [
        "pip install tensorflow-gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccaP6FLA92rf"
      },
      "source": [
        "#**Ram Check**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lig2LmFv9z3Q"
      },
      "outputs": [],
      "source": [
        "!cat /proc/meminfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDAcOuhw-Dcp"
      },
      "source": [
        "#**Data Gathering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGEi6EucX_dp"
      },
      "outputs": [],
      "source": [
        "base_directory = './NEW_100_5100dataset/'\n",
        "images_folder = os.path.join(base_directory, 'images')\n",
        "masks_folder = os.path.join(base_directory, 'masks')\n",
        "data = pd.read_csv(os.path.join(base_directory, 'output.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eC9PRLkARR_l"
      },
      "outputs": [],
      "source": [
        "base_directory = './NEW_3000data_5100dataset/3000_5100dataset_newmasks/'\n",
        "images_folder = os.path.join(base_directory, 'images')\n",
        "masks_folder = os.path.join(base_directory, 'masks')\n",
        "data = pd.read_csv(os.path.join(base_directory, 'output.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWcF7kPq4WGF"
      },
      "outputs": [],
      "source": [
        "#base_directory = './NEW_5100data_5106newmask/5108_5100dataset_newmasks/'\n",
        "base_directory = '/content/gdrive/MyDrive/Modern_Computer_Vision/directory_Plant_Segmentation/5100data_5100new'\n",
        "images_folder = os.path.join(base_directory, 'images')\n",
        "masks_folder = os.path.join(base_directory, 'masks')\n",
        "data = pd.read_csv(os.path.join(base_directory, 'output.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWeR6QecDUJp"
      },
      "outputs": [],
      "source": [
        "base_directory = './High_Quality_Data/'\n",
        "images_folder = os.path.join(base_directory, 'images')\n",
        "masks_folder = os.path.join(base_directory, 'masks')\n",
        "data = pd.read_csv(os.path.join(base_directory, 'meta_data.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8m52nefFnWrR"
      },
      "outputs": [],
      "source": [
        "base_directory = './High_Quality_Data/'\n",
        "images_folder = os.path.join(base_directory, 'images')\n",
        "masks_folder = os.path.join(base_directory, 'mask_reshape')\n",
        "data = pd.read_csv(os.path.join(base_directory, 'meta_data.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWxPjAnaPJn9"
      },
      "outputs": [],
      "source": [
        "base_directory = './low_quality_data/'\n",
        "images_folder = os.path.join(base_directory, 'images')\n",
        "masks_folder = os.path.join(base_directory, 'masks')\n",
        "data = pd.read_csv(os.path.join(base_directory, 'meta_data.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCTTZR0LFOdQ"
      },
      "outputs": [],
      "source": [
        "base_directory = './5100data_5100new/'\n",
        "images_folder = os.path.join(base_directory, 'images_new')\n",
        "masks_folder = os.path.join(base_directory, 'masks')\n",
        "data = pd.read_csv(os.path.join(base_directory, 'output.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTTHcGYyvBJ-"
      },
      "outputs": [],
      "source": [
        "base_directory = './5100data_w_new_mask/'\n",
        "images_folder = os.path.join(base_directory, 'images')\n",
        "masks_folder = os.path.join(base_directory, 'masks')\n",
        "data = pd.read_csv(os.path.join(base_directory, 'meta_data.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2xNhyDiAxN1"
      },
      "outputs": [],
      "source": [
        "base_directory = './data/Forest_Segmented/Forest_Segmented/'\n",
        "images_folder = os.path.join(base_directory, 'images_old')\n",
        "masks_folder = os.path.join(base_directory, 'masks')\n",
        "data = pd.read_csv(os.path.join(base_directory, 'meta_data.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrJlFUXP7LWY"
      },
      "outputs": [],
      "source": [
        "base_directory = './data_v2/'\n",
        "images_folder = os.path.join(base_directory, 'images')\n",
        "masks_folder = os.path.join(base_directory, 'masks')\n",
        "data = pd.read_csv(os.path.join(base_directory, 'meta_data.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-mWQSxhBALj"
      },
      "outputs": [],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_TrzJ5EBDck"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-J95Ty68Bd8r"
      },
      "outputs": [],
      "source": [
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwCIXOJdBfYy"
      },
      "outputs": [],
      "source": [
        "img_dim = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-bYLMN2vDo7"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpsKc6QJBgcS"
      },
      "outputs": [],
      "source": [
        "\n",
        "def input_target_split(data,images_folder,masks_folder,dim):\n",
        "    dataset = []\n",
        "    for index, row in data.iterrows():\n",
        "        image = load_img(os.path.join(images_folder, row['image']), target_size=(dim,dim))\n",
        "        mask = load_img(os.path.join(masks_folder, row['mask']), target_size=(dim,dim), color_mode='grayscale')\n",
        "        image = img_to_array(image)\n",
        "        image = image/255.0\n",
        "        mask = img_to_array(mask)\n",
        "        mask = mask/255.0\n",
        "        dataset.append((image,mask))\n",
        "        print(f\"\\rProgress: {index}\",end='')\n",
        "    random.shuffle(dataset)\n",
        "    X, Y = zip(*dataset)\n",
        "\n",
        "    return np.array(X),np.array(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsTJTA7exDZD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "def input_target_split(data, images_folder, masks_folder, dim):\n",
        "    dataset = []\n",
        "    problematic_images = []\n",
        "\n",
        "    for index, row in data.iterrows():\n",
        "        image_path = os.path.join(images_folder, row['image'])\n",
        "        mask_path = os.path.join(masks_folder, row['mask'])\n",
        "\n",
        "        try:\n",
        "            image = load_img(image_path, target_size=(dim, dim))\n",
        "            mask = load_img(mask_path, target_size=(dim, dim), color_mode='grayscale')\n",
        "\n",
        "            image = img_to_array(image)\n",
        "            image = image / 255.0\n",
        "\n",
        "\n",
        "            mask = img_to_array(mask)\n",
        "            mask = mask / 255.0\n",
        "\n",
        "            dataset.append((image, mask))\n",
        "            print(f\"\\rProgress: {index}\", end='')\n",
        "        except Exception as e:\n",
        "            problematic_images.append((index, image_path, mask_path, str(e)))\n",
        "\n",
        "    if problematic_images:\n",
        "        print(\"\\nProblematic Images:\")\n",
        "        for idx, image_path, mask_path, error_message in problematic_images:\n",
        "            print(f\"Index: {idx}, Image: {image_path}, Mask: {mask_path}, Error: {error_message}\")\n",
        "\n",
        "    random.shuffle(dataset)\n",
        "    X, Y = zip(*dataset)\n",
        "\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JulSd8CUBh67"
      },
      "outputs": [],
      "source": [
        "X, Y = input_target_split(data,images_folder,masks_folder,img_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Dga5kMRBjaS"
      },
      "outputs": [],
      "source": [
        "print(\"Image Dimensions: \",X.shape)\n",
        "print(\"Mask Dimensions: \",Y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDO2Ly80s0cf"
      },
      "source": [
        "#**Data Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcCOz4iVCI_D"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (15 , 9))\n",
        "n = 0\n",
        "for i in range(15):\n",
        "    n+=1\n",
        "    plt.subplot(5 , 5, n)\n",
        "    plt.subplots_adjust(hspace = 0.5 , wspace = 0.3)\n",
        "    plt.imshow(X[i])\n",
        "    plt.title('Image')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMjeduWHCL-P"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (15 , 9))\n",
        "n = 0\n",
        "for i in range(15):\n",
        "    n+=1\n",
        "    plt.subplot(5 , 5, n)\n",
        "    plt.subplots_adjust(hspace = 0.5 , wspace = 0.3)\n",
        "    plt.imshow(np.squeeze(Y[i]))\n",
        "    plt.title('Masks')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAblder0s5AO"
      },
      "source": [
        "#**Data Arrangement**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bUaknj2jZhZ"
      },
      "outputs": [],
      "source": [
        "split_1 = round(X.shape[0]*0.70)\n",
        "split_2 = round(X.shape[0]*0.90)\n",
        "X_train = X[:split_1]\n",
        "Y_train = Y[:split_1]\n",
        "X_val = X[split_1:split_2]\n",
        "Y_val = Y[split_1:split_2]\n",
        "X_test = X[split_2:]\n",
        "Y_test = Y[split_2:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XL6OwBSCMa5"
      },
      "outputs": [],
      "source": [
        "datagen = ImageDataGenerator()\n",
        "valgen = ImageDataGenerator()\n",
        "testgen = ImageDataGenerator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4saKegcCNyp"
      },
      "outputs": [],
      "source": [
        "datagen.fit(X_train)\n",
        "valgen.fit(X_val)\n",
        "testgen.fit(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4ClL9E7svlp"
      },
      "source": [
        "#**U-Net Arch Implementation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sA318fVyCSNC"
      },
      "outputs": [],
      "source": [
        "def conv_block(input, num_filters):\n",
        "    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def encoder_block(input, num_filters):\n",
        "    x = conv_block(input, num_filters)\n",
        "    p = MaxPool2D((2, 2))(x)\n",
        "    return x, p\n",
        "\n",
        "def decoder_block(input, skip_features, num_filters):\n",
        "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n",
        "    x = Concatenate()([x, skip_features])\n",
        "    x = conv_block(x, num_filters)\n",
        "    return x\n",
        "\n",
        "def build_unet(input_shape):\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    s1, p1 = encoder_block(inputs, 32)\n",
        "    s2, p2 = encoder_block(p1, 64)\n",
        "    s3, p3 = encoder_block(p2, 128)\n",
        "    s4, p4 = encoder_block(p3, 256)\n",
        "\n",
        "    b1 = conv_block(p4, 512)\n",
        "\n",
        "    d1 = decoder_block(b1, s4, 256)\n",
        "    d2 = decoder_block(d1, s3, 128)\n",
        "    d3 = decoder_block(d2, s2, 64)\n",
        "    d4 = decoder_block(d3, s1, 32)\n",
        "\n",
        "    outputs = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d4)\n",
        "\n",
        "    model = Model(inputs, outputs, name=\"U-Net\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeP4jiSi5j5G"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, regularizers\n",
        "from tensorflow.keras import backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SimWZPIy4Gf9"
      },
      "outputs": [],
      "source": [
        "def dice_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2.0 * intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1.0)\n",
        "\n",
        "\n",
        "def iou_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
        "\n",
        "\n",
        "def iou_coef_loss(y_true, y_pred):\n",
        "    return -iou_coef(y_true, y_pred)\n",
        "\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return -dice_coef(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MP06V_a0CU5q"
      },
      "outputs": [],
      "source": [
        "input_shape = (img_dim, img_dim, 3)\n",
        "model = build_unet(input_shape)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHaL03xbKH-9"
      },
      "source": [
        "#**Dice or IoU Coef**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlnrgcYvCXHh"
      },
      "outputs": [],
      "source": [
        "#model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001), loss = ['binary_crossentropy'], metrics=[iou_coef,'accuracy'])\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(lr = 1e-3), loss=dice_coef_loss, metrics=['accuracy', iou_coef])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAglHQn-KMkQ"
      },
      "source": [
        "#**Dice Coef**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSm7yiA8KxF_"
      },
      "outputs": [],
      "source": [
        "def dice_coef_loss(y_true,y_pred):\n",
        "  y_true_f=K.flatten(y_true)\n",
        "  y_pred_f=K.flatten(y_pred)\n",
        "  intersection=K.sum(y_true_f*y_pred_f)\n",
        "  return 1-(2*intersection)/(K.sum(y_true_f*y_true_f)+K.sum(y_pred_f*y_pred_f))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGVLbo03KGQe"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001), loss = dice_coef_loss, metrics=[iou_coef,'accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m29cnV7esshX"
      },
      "source": [
        "#**Training Part**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7lGroR1vCYRi"
      },
      "outputs": [],
      "source": [
        "model_path = \"unet.h5\"\n",
        "checkpoint = ModelCheckpoint(model_path,\n",
        "                             monitor=\"val_loss\",\n",
        "                             mode=\"min\",\n",
        "                             save_best_only = True,\n",
        "                             verbose=1)\n",
        "\n",
        "earlystop = EarlyStopping(monitor = 'val_loss',\n",
        "                          min_delta = 0,\n",
        "                          patience = 9,\n",
        "                          verbose = 1,\n",
        "                          restore_best_weights = True)\n",
        "hist = model.fit(X_train, Y_train,\n",
        "                    verbose=1,\n",
        "                    batch_size = 10,\n",
        "                    validation_data=(X_val, Y_val),\n",
        "                    shuffle=False,\n",
        "                    epochs=70)\n",
        "model.save('UNet_50epochs.hdf5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTdRb-eGspTv"
      },
      "source": [
        "#**Performance Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTEc7odcljZx"
      },
      "outputs": [],
      "source": [
        "history_dict = hist.history\n",
        "history_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1PmGL4blou-"
      },
      "outputs": [],
      "source": [
        "# Plotting our loss charts\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Use the History object we created to get our saved performance results\n",
        "history_dict = hist.history\n",
        "\n",
        "# Extract the loss and validation losses\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "\n",
        "# Get the number of epochs and create an array up to that number using range()\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "\n",
        "# Plot line charts for both Validation and Training Loss\n",
        "line1 = plt.plot(epochs, val_loss_values, label='Validation Loss')\n",
        "line2 = plt.plot(epochs, loss_values, label='Training Loss')\n",
        "#plt.setp(line1, linewidth=2.0, marker = '+', markersize=10.0)\n",
        "#plt.setp(line2, linewidth=2.0, marker = '4', markersize=10.0)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fR45Oh4ElsvA"
      },
      "outputs": [],
      "source": [
        "# Plotting our accuracy charts\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = hist.history\n",
        "\n",
        "acc_values = history_dict['accuracy']\n",
        "val_acc_values = history_dict['val_accuracy']\n",
        "epochs = range(1, len(acc_values) + 1)\n",
        "\n",
        "line1 = plt.plot(epochs, val_acc_values, label='Validation Accuracy')\n",
        "line2 = plt.plot(epochs, acc_values, label='Training Accuracy')\n",
        "#plt.setp(line1, linewidth=2.0, marker = '+', markersize=10.0)\n",
        "#plt.setp(line2, linewidth=2.0, marker = '4', markersize=10.0)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wba19EONLdUH"
      },
      "outputs": [],
      "source": [
        "# Plotting our loss charts\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Use the History object we created to get our saved performance results\n",
        "history_dict = hist.history\n",
        "\n",
        "# Extract the loss and validation losses\n",
        "loss_values = history_dict['iou_coef']\n",
        "val_loss_values = history_dict['val_iou_coef']\n",
        "\n",
        "# Get the number of epochs and create an array up to that number using range()\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "\n",
        "# Plot line charts for both Validation and Training Loss\n",
        "line1 = plt.plot(epochs, val_loss_values, label='Validation IoU')\n",
        "line2 = plt.plot(epochs, loss_values, label='Training IoU')\n",
        "plt.setp(line1, linewidth=2.0, marker = '+', markersize=10.0)\n",
        "plt.setp(line2, linewidth=2.0, marker = '4', markersize=10.0)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('IoU')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rz5yH1nHshLQ"
      },
      "source": [
        "#**Save the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uR1NCwY1l244",
        "outputId": "a1029880-04c9-4930-c5f5-4b66a3207277"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Saved\n"
          ]
        }
      ],
      "source": [
        "model.save(\"U-Net_Model.h5\")\n",
        "print(\"Model Saved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSLQ7XsumRlQ"
      },
      "outputs": [],
      "source": [
        "model.save_weights(\"U-Net_Model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1L8eu23gmS6S"
      },
      "outputs": [],
      "source": [
        "model.load_weights(\"U-Net_Model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNDGCDEPseEe"
      },
      "source": [
        "#**Visualization of Results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0i1_3lQuCemz"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (15 , 9))\n",
        "result = model.predict(X_test)\n",
        "output = result[0]\n",
        "output[output >= 0.5] = 1\n",
        "output[output < 0.5] = 0\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(X_test[0])\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(np.squeeze(Y_test[0]))\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(np.squeeze(output))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RLu69HtVCfnS"
      },
      "outputs": [],
      "source": [
        "f, axarr = plt.subplots(10,3,figsize=(20, 60))\n",
        "\n",
        "for i in range(0,10):\n",
        "    output = result[i]\n",
        "    output[output >= 0.5] = 1\n",
        "    output[output < 0.5] = 0\n",
        "\n",
        "    axarr[i,0].imshow(X_test[i])\n",
        "    axarr[i,0].title.set_text('Original Image')\n",
        "    axarr[i,1].imshow(np.squeeze(Y_test[i]))\n",
        "    axarr[i,1].title.set_text('Actual Mask')\n",
        "    axarr[i,2].imshow(np.squeeze(output))\n",
        "    axarr[i,2].title.set_text('Predicted Mask')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vstqir4asa5w"
      },
      "source": [
        "#**TF_LITE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFzz2z4mCaVq"
      },
      "outputs": [],
      "source": [
        "#tf_lite convert\n",
        "TF_LITE_MODEL_FILE_NAME = \"tf_lite_model.tflite\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiaK1r_-CbHD"
      },
      "outputs": [],
      "source": [
        "tf_lite_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = tf_lite_converter.convert()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WAtBaU6Cc4K"
      },
      "outputs": [],
      "source": [
        "tflite_model_name = TF_LITE_MODEL_FILE_NAME\n",
        "open(tflite_model_name, \"wb\").write(tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_qNSMpZChkU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Create a model using high-level tf.keras.* APIs\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(units=1, input_shape=[1]),\n",
        "    tf.keras.layers.Dense(units=16, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=1)\n",
        "])\n",
        "model.compile(optimizer='sgd', loss='mean_squared_error') # compile the model\n",
        "model.fit(x=[-1, 0, 1], y=[-3, -1, 1], epochs=50) # train the model\n",
        "# (to generate a SavedModel) tf.saved_model.save(model, \"saved_model_keras_dir\")\n",
        "\n",
        "# Convert the model.\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model.\n",
        "with open('model.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "vstqir4asa5w"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}